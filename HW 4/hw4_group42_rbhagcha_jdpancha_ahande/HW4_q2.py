# -*- coding: utf-8 -*-
"""HW4_jpancha_ahande_raunak.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AvCXjiYI0pMfxc7Zd3U5-fDS0efGAecO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

def load_data(path, test_size, dataset):

      
    if dataset == "letter":
        df = pd.read_csv(path)
        le = LabelEncoder()
        df = df.loc[df['T'].isin(['C','G'])]
        df['T'] = le.fit_transform(df['T'])
        df['T'] = df['T'].replace(0,-1)

        y = df['T'].values
        x = df.iloc[:,1:].values
    elif dataset == "spam_ham":
        df = pd.read_csv(path, header=None)
        y = df[57]
        x = df.drop(columns=57)
    
    else:
        df = pd.read_csv(path, header=None, sep=' ')
        le = LabelEncoder()
        y = df[20]
        x = df.drop(columns=20)
        for clms in x.columns:
            x[clms] = le.fit_transform(x[clms])
         
    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size = test_size,
        random_state=42)
    
    return x_train, x_test, y_train, y_test

class Model:

    def __init__(self, T, dataset, path, model):
        self.model = model
        self.dataset = dataset
        self.path = path
        self.T = T
        self.x_train, self.x_test, self.y_train, self.y_test = load_data(path=self.path, test_size=0.3, dataset=self.dataset)
        self.trees = []
 

    def bagging(self, bootstraps):

        np.random.seed(100)
        N, D = self.x_train.shape
        train_size, test_size = len(self.x_train), len(self.x_test) 

        bagging_train_error= []
        bagging_test_error = [] 

        for bootstrap in range(bootstraps):
            bootstrap_sample = np.random.choice(np.arange(N), size = N, replace = True)
            
            x_train_sample = self.x_train[bootstrap_sample]
            y_train_sample = self.y_train[bootstrap_sample]
            

            tree = DecisionTreeClassifier(max_depth = 1, random_state = 1)

            tree.fit(x_train_sample, y_train_sample)

            self.trees.append(tree)

            pred_train, pred_test = [np.zeros(train_size), np.zeros(test_size)]

            train_preds_i = tree.predict(x_train_sample)
            test_preds_i = tree.predict(self.x_test)

            pred_train = [sum(k) for k in zip(pred_train, 
                                            [i for i in train_preds_i])]
            pred_test = [sum(x) for x in zip(pred_test, 
                                            [i for i in test_preds_i])]

        
            pred_train, pred_test = np.sign(pred_train), np.sign(pred_test) 

            train_error = self.compute_err_rate(pred_train, y_train_sample)
            test_error = self.compute_err_rate(pred_test, self.y_test)

            bagging_train_error.append(train_error)
            bagging_test_error.append(test_error)

        
        return np.mean(bagging_train_error), \
                np.mean(bagging_test_error)

 
    def compute_err_rate(self, y_pred, y_true):
        return sum(y_pred!=y_true)/float(len(y_true))

    
    def DTClassifier(self):
        clf = self.model
        clf.fit(self.x_train,self.y_train)
        pred_train = clf.predict(self.x_train)
        pred_test = clf.predict(self.x_test)
        return self.compute_err_rate(pred_train, self.y_train), \
           self.compute_err_rate(pred_test, self.y_test)
    
    def adaboost(self, M):
        train_size, test_size = len(self.x_train), len(self.x_test) 

        w_i = np.ones(train_size) / test_size

        # initializing arrays of size train and test
        pred_train, pred_test = [np.zeros(train_size), np.zeros(test_size)]
        
        clf = self.model
        if M == 0 or M is None:
            M = self.T 

        for i in range(M):
            
            clf.fit(self.x_train, self.y_train, sample_weight = w_i)
            
            train_preds_i = clf.predict(self.x_train)
            test_preds_i = clf.predict(self.x_test)
            
            # Indicator function
            misclassifications = [int(i) for i in (train_preds_i != self.y_train)]
            
            # penalize if you miss
            missclassification2 = [i if i==1 else -1 for i in misclassifications]
            
            # Error
            err = np.dot(w_i,misclassifications) / sum(w_i)

            # Alpha
            epsilon = 1e-10
            alpha_t = 0.5 * np.log( (1 - err + epsilon) / float(err + epsilon))

            # Update weights
            w_i = np.multiply(w_i, np.exp([float(i) * alpha_t for i in missclassification2]))

            # Add to prediction list

            pred_train = [sum(k) for k in zip(pred_train, 
                                            [i * alpha_t for i in train_preds_i])]
            pred_test = [sum(x) for x in zip(pred_test, 
                                            [i * alpha_t for i in test_preds_i])]

        
        pred_train, pred_test = np.sign(pred_train), np.sign(pred_test) 

        return self.compute_err_rate(pred_train, self.y_train), \
            self.compute_err_rate(pred_test, self.y_test)
        
    def plot(self, train_error, test_error):

        df_error = pd.DataFrame([train_error, test_error]).T
        df_error.columns = ['Training Error', 'Test Error']
        plot1 = df_error.plot(linewidth = 4, figsize = (10,8),
                color = ['darkblue', 'green'], grid = True)
        plot1.set_xlabel('Number of iterations', fontsize = 14)
        plot1.set_xticklabels(range(0,450,50))
        plot1.set_ylabel('Error rate', fontsize = 14)
        plot1.set_title('Error rate vs number of iterations', fontsize = 18)
        plt.axhline(y=train_error[0], linewidth=1, color = 'blue', ls = 'dashed')

"""## Boosting

### Letter Recognition Dataset
"""

model = DecisionTreeClassifier(max_depth = 1, random_state = 1) 
boost = Model(T=10, dataset="letter", path='letter-recognition.data', model=model) 
errors= boost.DTClassifier()
train_error, test_error = [errors[0]], [errors[1]]

x_range = range(10, 410, 10)
for i in x_range:    
    error_i = boost.adaboost(M=i)
    train_error.append(error_i[0])
    test_error.append(error_i[1]) 
boost.plot(train_error, test_error)
# The error rates decrease as we increase the number of trees

"""### Spam dataset"""

model = DecisionTreeClassifier(max_depth = 1, random_state = 1) 
boost = Model(T=10, dataset="spam_ham", path='spambase.data', model=model) 
errors= boost.DTClassifier()
train_error, test_error = [errors[0]], [errors[1]]

x_range = range(10, 410, 10)
for i in x_range:    
    error_i = boost.adaboost(M=i)
    train_error.append(error_i[0])
    test_error.append(error_i[1]) 
boost.plot(train_error, test_error)

# The training and testing error is not increasing for boosting

"""### German Credit dataset"""

model = DecisionTreeClassifier(max_depth = 1, random_state = 1) 
boost = Model(T=10, dataset="german", path='german.data', model=model) 
errors= boost.DTClassifier()
train_error, test_error = [errors[0]], [errors[1]]

x_range = range(10, 410, 10)
for i in x_range:    
    error_i = boost.adaboost(M=i)
    train_error.append(error_i[0])
    test_error.append(error_i[1]) 
boost.plot(train_error, test_error)
# The error rates decrease as we increase the number of trees

"""## Bagging

### German Credit dataset
"""

model = Model(T=10, dataset="letter", path='letter-recognition.data', model=model) 
 
x_range = range(10,410,10)
train_error, test_error = [], []
for i in x_range:
    train_e, test_e = model.bagging(bootstraps=i)
    train_error.append(train_e)
    test_error.append(test_e)
model.plot(train_error, test_error)

# The model is overfitting (but bagging is a better estimate)

# It is a better estimate for boosting

"""### Letter-Recognition """

model = Model(T=10, dataset="german", path='german.data', model=model) 
 
x_range = range(10,410,10)
train_error, test_error = [], []
for i in x_range:
    train_e, test_e = model.bagging(bootstraps=i)
    train_error.append(train_e)
    test_error.append(test_e)
model.plot(train_error, test_error)

# The model is overfitting (but bagging is a better estimate)

"""### Spam"""

model = Model(T=10, dataset="spam_ham", path='spambase.data', model=model) 
 
x_range = range(10,410,10)
train_error, test_error = [], []
for i in x_range:
    train_e, test_e = model.bagging(bootstraps=i)
    train_error.append(train_e)
    test_error.append(test_e)
model.plot(train_error, test_error)

# The model is overfitting (but bagging is a better estimate)

"""### Bagging is a better estimate, as it averages all trees' predictions and hence can mitigate bias as well.

## References

1) https://dafriedman97.github.io/mlbook/content/c6/s2/bagging.html

2) https://contactsunny.medium.com/how-to-split-your-dataset-to-train-and-test-datasets-using-scikit-learn-e7cf6eb5e0d

3) https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html

4) https://music.apple.com/us/listen-now

5) https://github.com/SiluPanda/ensemble-learning-bagging-and-boosting

6) https://machinelearningmastery.com/implement-bagging-scratch-python/
"""

